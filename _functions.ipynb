{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b091b4-8dd1-441b-85b8-397211e9a8f0",
     "showTitle": true,
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading enviroment setup function...\")\n",
    "# Import libraries\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Whole pipeline as CDF\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", True)\n",
    "\n",
    "#Setting path objects\n",
    "main_path = \"dbfs:/mnt/borges_portifolio\"\n",
    "files_path = f\"{main_path}/auto_loader_files/source_folder\"\n",
    "source = f\"{main_path}/auto_loader_files/stream_files\"\n",
    "\n",
    "# Estabilishing the folder where Auto Loader will maintain identifying information of the stream\n",
    "checkpoint_path = f'{main_path}/_checkpoints'\n",
    "\n",
    "# Defining data type of each column for each JSON file or nested data\n",
    "bronze_schema = \"key BINARY, value BINARY, topic STRING, partition INT, offset INT, timestamp LONG\"\n",
    "\n",
    "heartrate_schema = \"device_id INTEGER, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "workout_json_schema = \"user_id INTEGER, workout_id INTEGER, timestamp DOUBLE, action STRING, session_id INTEGER\"\n",
    "\n",
    "def env_setup():\n",
    "    # Creating Bronze schema\n",
    "    dbutils.fs.rm(\"dbfs:/user/hive/warehouse/bronze.db\", True) # Clearing previous data \n",
    "    spark.sql(\"DROP SCHEMA IF EXISTS bronze CASCADE;\") # Dropping schema\n",
    "    spark.sql(\"CREATE SCHEMA bronze;\") # Creating schema\n",
    "    spark.catalog.setCurrentDatabase('bronze') # Setting the bronze schema as default for this notebook\n",
    "\n",
    "    # Creating Silver schema\n",
    "    dbutils.fs.rm(\"dbfs:/user/hive/warehouse/silver.db\", True)\n",
    "    spark.sql(\"DROP SCHEMA IF EXISTS silver CASCADE;\")\n",
    "    spark.sql(\"CREATE SCHEMA silver;\")\n",
    "\n",
    "    # Creating Gold schema\n",
    "    dbutils.fs.rm(\"dbfs:/user/hive/warehouse/gold.db\", True)\n",
    "    spark.sql(\"DROP SCHEMA IF EXISTS gold CASCADE;\")\n",
    "    spark.sql(\"CREATE SCHEMA gold;\")\n",
    "\n",
    "    # Cleaning any structured streaming or Auto Loader checkpoints\n",
    "    dbutils.fs.rm(\"dbfs:/mnt/borges_portifolio/_checkpoints\", True) # Clearing checkpoints\n",
    "\n",
    "    dbutils.fs.mkdirs(\"dbfs:/mnt/borges_portifolio/auto_loader_files/source_folder\")\n",
    "    \n",
    "    # Setting parallelism\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n",
    "\n",
    "    # Clone gym_logs table to Silver schema \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.gym_logs\n",
    "    SHALLOW CLONE delta.`{main_path}/gym_logs`\n",
    "    LOCATION 'dbfs:/user/hive/warehouse/silver.db/gym_logs'\n",
    "    \"\"\")\n",
    "\n",
    "    # Clone user_lookup table to Silver schema \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver.user_lookup\n",
    "    SHALLOW CLONE delta.`{main_path}/user-lookup`\n",
    "    LOCATION 'dbfs:/user/hive/warehouse/silver.db/user_lookup'\n",
    "    \"\"\")\n",
    "\n",
    "    # Enable CDF on user_lookup\n",
    "    spark.sql(\"ALTER TABLE silver.user_lookup SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\")\n",
    "\n",
    "\n",
    "\n",
    "time.sleep(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b748c45f-4a39-4264-a51d-3c8d17752f90",
     "showTitle": true,
     "title": "Bronze"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading Bronze related functions...\")\n",
    "\n",
    "# Creating the function that will ingest the files into the Bronze table\n",
    "def process_bronze():\n",
    "    \n",
    "    # Time traveling to obtain the date_lookup in a DataFrame\n",
    "    date_lookup_df = spark.read.option(\"versionAsOf\", 0).load(f\"{main_path}/date_lookup\").select(\"date\", \"week_part\")\n",
    "\n",
    "    (spark.readStream\n",
    "                  .format(\"cloudFiles\")\n",
    "                  .schema(bronze_schema)\n",
    "                  .option(\"cloudFiles.format\", \"json\")\n",
    "                  .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}/bronze_schema\") # Location to store schema information\n",
    "                  .load(files_path) # Source Folder\n",
    "                  .join(f.broadcast(date_lookup_df), f.to_date((f.col(\"timestamp\")/1000).cast(\"timestamp\")) == f.col(\"date\"), \"left\")\n",
    "                  .writeStream\n",
    "                        .option(\"checkpointLocation\", f\"{checkpoint_path}/bronze\") # Location to store checkpoint information\n",
    "                        .partitionBy(\"topic\", \"week_part\") # Partitioning by topic and week_part to increase performance\n",
    "                        .trigger(processingTime='5 seconds')\n",
    "                        .queryName(\"bronze\")\n",
    "                        .table(\"bronze\"))\n",
    "\n",
    "# Creating a function for reprocessing all the files into the Bronze table\n",
    "def reprocess_bronze():\n",
    "    spark.sql(\"DROP TABLE IF EXISTS bronze.bronze\")\n",
    "\n",
    "    dbutils.fs.rm(f\"{checkpoint_path}/bronze\", True)\n",
    "    \n",
    "    process_bronze()\n",
    "\n",
    "time.sleep(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79b72ed3-999b-4873-b379-a4a101303e5e",
     "showTitle": true,
     "title": "Heartrate"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading Heartrate related functions...\")\n",
    "\n",
    "def heartrate_merge_data(df, batch_id): \n",
    "    df.createOrReplaceTempView(\"stream_updates\")    \n",
    "    sql_query = \"\"\"\n",
    "    MERGE INTO silver.heart_rate a\n",
    "    USING stream_updates b\n",
    "    ON a.device_id=b.device_id AND a.time=b.time\n",
    "    WHEN MATCHED THEN\n",
    "    UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN\n",
    "    INSERT * \n",
    "    \"\"\"    \n",
    "    df.sparkSession.sql(sql_query)\n",
    "\n",
    "def process_heart_rate():\n",
    "    # Create heart_rate table\n",
    "    spark.sql(\"CREATE TABLE IF NOT EXISTS silver.heart_rate (device_id INTEGER, time TIMESTAMP, heartrate DOUBLE) USING DELTA\")\n",
    "\n",
    "    # Define constraints based on previous analysis\n",
    "    spark.sql(\"ALTER TABLE silver.heart_rate ADD CONSTRAINT validbpm CHECK (heartrate > 0)\")\n",
    "    spark.sql(\"ALTER TABLE silver.heart_rate ADD CONSTRAINT check_date CHECK (time > '2019-11-30')\")\n",
    "\n",
    "    # Structured Streaming query\n",
    "    (spark.readStream\n",
    "            .table(\"bronze\")\n",
    "            .filter(\"topic = 'bpm'\")\n",
    "            .select(f.from_json(f.col(\"value\").cast(\"string\"), heartrate_schema).alias(\"bpm\"))\n",
    "            .filter(\"bpm.heartrate > 0\")\n",
    "            .select(\"bpm.*\")\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "            .dropDuplicates()\n",
    "            .writeStream\n",
    "                .foreachBatch(heartrate_merge_data) # Apply merge logic to each batch\n",
    "                .option(\"checkpointLocation\", f\"{checkpoint_path}/heart_rate\") # Set checkpoint location\n",
    "                .outputMode(\"update\") # Allow changing existing data in the table \n",
    "                .trigger(processingTime='5 seconds') # Process all available data, multiple batches if needed\n",
    "                .queryName(\"silver_heart_rate\") # Name the query for easy reference\n",
    "                .start() # Start the query\n",
    "                ) # Wait for query to finish\n",
    "\n",
    "def reprocess_heart_rate():\n",
    "\n",
    "    spark.sql(\"DROP TABLE IF EXISTS silver.heart_rate\")\n",
    "\n",
    "    dbutils.fs.rm(f\"{checkpoint_path}/heart_rate\", True)\n",
    "    \n",
    "    process_heart_rate()\n",
    "\n",
    "time.sleep(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a8182be-74b0-4d33-b9dc-c342469b0997",
     "showTitle": true,
     "title": "Workouts"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading Workout related functions...\")\n",
    "\n",
    "def workout_merge_data(df, batch_id):\n",
    "    df.createOrReplaceTempView(\"stream_updates\")\n",
    "\n",
    "    sql_query = \"\"\"\n",
    "    MERGE INTO silver.workouts a\n",
    "    USING stream_updates b\n",
    "    ON a.user_id = b.user_id AND a.time = b.time\n",
    "    WHEN MATCHED THEN\n",
    "    UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "    df.sparkSession.sql(sql_query)\n",
    "\n",
    "def process_workouts():\n",
    "    # Create the workouts table if it doesn't exist\n",
    "    spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS \n",
    "                    silver.workouts ( \n",
    "                        user_id     INTEGER,   \n",
    "                        workout_id  INTEGER,   \n",
    "                        time        TIMESTAMP, \n",
    "                        action      STRING,    \n",
    "                        session_id  INTEGER    \n",
    "                                    ) \n",
    "                USING DELTA\n",
    "                \"\"\")\n",
    "\n",
    "    # Add constraint to ensure timestamp is after a specific date\n",
    "    spark.sql(f\"ALTER TABLE silver.workouts ADD CONSTRAINT check_date CHECK (time > '2019-11-30')\")\n",
    "\n",
    "    # Read stream from the bronze table, apply transformations, and write to the workouts table\n",
    "    (spark.readStream\n",
    "            .table(\"bronze.bronze\")\n",
    "            .filter(\"topic = 'workout'\")\n",
    "            .select(f.from_json(f.col(\"value\").cast(\"string\"), workout_json_schema).alias(\"wkrt\"))\n",
    "            .select(\"wkrt.*\")\n",
    "            .select(\"user_id\", \"workout_id\", f.col(\"timestamp\").cast(\"timestamp\").alias(\"time\"), \"action\", \"session_id\")\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"time\"])\n",
    "                .writeStream\n",
    "                .foreachBatch(workout_merge_data)\n",
    "                .outputMode(\"update\")\n",
    "                .option(\"checkpointLocation\", f\"{checkpoint_path}/workouts\")\n",
    "                .queryName(\"silver_workouts\")\n",
    "                .trigger(processingTime='5 seconds')\n",
    "                .start()\n",
    "                )\n",
    "\n",
    "def reprocess_workouts():\n",
    "    # Drop the workouts table if it exists\n",
    "    spark.sql(f\"drop table if exists silver.workouts\")\n",
    "\n",
    "    # Remove checkpoint location for workouts\n",
    "    dbutils.fs.rm(f\"{checkpoint_path}/workouts\", True)\n",
    "\n",
    "    # Reprocess workouts\n",
    "    process_workouts()\n",
    "\n",
    "time.sleep(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7af3b92-9461-4c66-9e9e-384b0280b0e2",
     "showTitle": true,
     "title": "Completed Workouts"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading Completed Workouts related functions...\")\n",
    "\n",
    "def completed_wrkt_merge_data(df, batch_id):\n",
    "    df.createOrReplaceTempView(\"stream_updates\")\n",
    "\n",
    "    sql_query = \"\"\"\n",
    "    MERGE INTO silver.completed_workouts a\n",
    "    USING stream_updates b\n",
    "    ON a.user_id=b.user_id AND a.session_id=b.session_id\n",
    "    WHEN MATCHED THEN\n",
    "    UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN\n",
    "    INSERT * \n",
    "    \"\"\"    \n",
    "    df.sparkSession.sql(sql_query)\n",
    "\n",
    "\n",
    "def process_completed_workouts():\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS  \n",
    "      silver.completed_workouts   \n",
    "        (user_id      INTEGER,    \n",
    "        session_id   INTEGER,    \n",
    "        start_time   TIMESTAMP,  \n",
    "        end_time     TIMESTAMP)  \n",
    "      USING DELTA \\\n",
    "    \"\"\")\n",
    "\n",
    "    (spark.readStream\n",
    "          .table(\"silver.workouts\")\n",
    "          .groupBy(\"user_id\", \"session_id\")\n",
    "          .agg(\n",
    "              f.max(f.when(f.col(\"action\") == \"start\", f.col(\"time\"))).alias(\"start_time\"),\n",
    "              f.max(f.when(f.col(\"action\") == \"stop\", f.col(\"time\"))).alias(\"end_time\")\n",
    "              )\n",
    "          .filter(f.col(\"start_time\").isNotNull() & f.col(\"end_time\").isNotNull())\n",
    "          .select(\"user_id\", \"session_id\", \"start_time\", \"end_time\")\n",
    "          .withWatermark(\"start_time\", \"30 seconds\")\n",
    "          .writeStream\n",
    "              .foreachBatch(completed_wrkt_merge_data)\n",
    "              .option(\"checkpointLocation\", f\"{checkpoint_path}/completed_workouts\")\n",
    "              .outputMode(\"update\")\n",
    "              .trigger(processingTime='5 seconds')\n",
    "              .queryName(\"silver_completed_workouts\")\n",
    "              .start()\n",
    "              \n",
    "    )\n",
    "\n",
    "def reprocess_completed_workouts():\n",
    "    dbutils.fs.rm(f\"{checkpoint_path}/completed_workouts\", True)\n",
    "\n",
    "    spark.sql(\"drop table if exists silver.completed_workouts\")\n",
    "\n",
    "    process_completed_workouts()    \n",
    "\n",
    "time.sleep(0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2c729a-96f8-4597-80e0-82f72c193e61",
     "showTitle": true,
     "title": "Gym Report"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading Gym Report related functions...\")\n",
    "\n",
    "def process_gym_reports():\n",
    "  spark.sql(\"\"\"\n",
    "            DROP VIEW if EXISTS gold.gym_report;\n",
    "            \"\"\")\n",
    "\n",
    "  spark.sql(\"\"\"\n",
    "                CREATE OR REPLACE VIEW gold.gym_report AS (\n",
    "                  SELECT gym, mac_address, date, workouts, (last_timestamp - first_timestamp)/60 minutes_in_gym, (to_unix_timestamp(end_workout) - to_unix_timestamp(start_workout))/60 minutes_exercising\n",
    "                  FROM silver.gym_logs c\n",
    "                  INNER JOIN (\n",
    "                    SELECT b.mac_address, to_date(start_time) date, collect_set(session_id) workouts, min(start_time) start_workout, max(end_time) end_workout\n",
    "                    FROM silver.completed_workouts a\n",
    "                    INNER JOIN silver.user_lookup b\n",
    "                    ON a.user_id = b.user_id\n",
    "                    GROUP BY mac_address, to_date(start_time)\n",
    "                    ) d\n",
    "                    ON c.mac = d.mac_address AND to_date(CAST(c.first_timestamp AS timestamp)) = d.date\n",
    "                );\n",
    "            \"\"\")\n",
    "\n",
    "time.sleep(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49bd753-9182-413c-87a4-847c8fae1efe",
     "showTitle": true,
     "title": "Users"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading Users related functions...\")\n",
    "\n",
    "def users_merge_crt_dlt(microBatchDF, batchId):\n",
    "\n",
    "    window = Window.partitionBy(\"alt_id\").orderBy(f.col(\"updated\").desc())\n",
    "\n",
    "    (microBatchDF\n",
    "        .filter(f.col(\"update_type\").isin([\"new\", \"update\"]))\n",
    "        .withColumn(\"rank\", f.rank().over(window))\n",
    "        .filter(\"rank == 1\")\n",
    "        .drop(\"rank\")\n",
    "        .createOrReplaceTempView(\"ranked_updates\"))\n",
    "\n",
    "    microBatchDF.sparkSession.sql(\"\"\"\n",
    "        MERGE INTO silver.users u\n",
    "        USING ranked_updates r\n",
    "        ON u.alt_id=r.alt_id\n",
    "        WHEN MATCHED AND u.updated < r.updated\n",
    "          THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED\n",
    "          THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "    (microBatchDF\n",
    "        .filter(\"update_type = 'delete'\")\n",
    "        .select(f.col(\"alt_id\"), \n",
    "                f.col(\"updated\").alias(\"requested\"), \n",
    "                f.date_add(\"updated\", 30).alias(\"deadline\"), \n",
    "                f.lit(\"requested\").alias(\"status\"))\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"txnVersion\", batchId)\n",
    "        .option(\"txnAppId\", \"batch_rank_upsert\")\n",
    "        .saveAsTable(\"silver.delete_requests\"))\n",
    "    \n",
    "def process_users():\n",
    "\n",
    "    salt = \"BEANS\"\n",
    "\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS silver.users (alt_id STRING, user_id INTEGER, dob DATE, sex STRING, gender STRING, first_name STRING, last_name STRING, street_address STRING, city STRING, state STRING, zip INT, updated TIMESTAMP) USING DELTA\")\n",
    "\n",
    "    users_schema = \"\"\"\n",
    "        user_id INTEGER, \n",
    "        update_type STRING, \n",
    "        timestamp FLOAT, \n",
    "        dob STRING, \n",
    "        sex STRING, \n",
    "        gender STRING, \n",
    "        first_name STRING, \n",
    "        last_name STRING, \n",
    "        address STRUCT<\n",
    "            street_address: STRING, \n",
    "            city: STRING, \n",
    "            state: STRING, \n",
    "            zip: INT\n",
    "    >\"\"\"\n",
    "    \n",
    "    (spark.readStream\n",
    "        .table(\"bronze.bronze\")\n",
    "        .filter(\"topic = 'user_info'\")\n",
    "        .dropDuplicates()\n",
    "        .select(f.from_json(f.col(\"value\").cast(\"string\"), users_schema).alias(\"users\"))\n",
    "        .select(\"users.*\")\n",
    "        .select(f.sha2(f.concat(f.col(\"user_id\"), f.lit(f\"{salt}\")), 256).alias(\"alt_id\"),\n",
    "                f.col(\"user_id\"),\n",
    "                f.col('timestamp').cast(\"timestamp\").alias(\"updated\"),\n",
    "                f.to_date('dob','MM/dd/yyyy').alias('dob'),\n",
    "                'sex', \n",
    "                'gender',\n",
    "                'first_name',\n",
    "                'last_name',\n",
    "                'address.*', \n",
    "                'update_type')\n",
    "        .writeStream\n",
    "        .foreachBatch(users_merge_crt_dlt)\n",
    "        .outputMode(\"update\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_path}/users\")\n",
    "        .queryName(\"silver_users\") # Name the query for easy reference\n",
    "        .trigger(processingTime='5 seconds')\n",
    "        .start()\n",
    "        )\n",
    "        \n",
    "def reprocess_users():\n",
    "    dbutils.fs.rm(f\"{checkpoint_path}/users\", True)\n",
    "    dbutils.fs.rm(f\"{main_path}/delete_requests\", True)\n",
    "\n",
    "    spark.sql(\"drop table if exists silver.users\")\n",
    "    spark.sql(\"drop table if exists silver.delete_requests\")\n",
    "\n",
    "    process_users()\n",
    "\n",
    "time.sleep(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd2f898-f5fe-4eaf-8932-d686d7e87c4b",
     "showTitle": true,
     "title": "User Bins"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading User Bins related functions...\")\n",
    "\n",
    "def age_bins(dob_col):\n",
    "    age_col = f.floor(f.months_between(f.current_date(), dob_col)/12).alias(\"age\")\n",
    "    \n",
    "    return (f.when((age_col < 18), \"under 18\")\n",
    "             .when((age_col >= 18) & (age_col < 25), \"18-25\")\n",
    "             .when((age_col >= 25) & (age_col < 35), \"25-35\")\n",
    "             .when((age_col >= 35) & (age_col < 45), \"35-45\")\n",
    "             .when((age_col >= 45) & (age_col < 55), \"45-55\")\n",
    "             .when((age_col >= 55) & (age_col < 65), \"55-65\")\n",
    "             .when((age_col >= 65) & (age_col < 75), \"65-75\")\n",
    "             .when((age_col >= 75) & (age_col < 85), \"75-85\")\n",
    "             .when((age_col >= 85) & (age_col < 95), \"85-95\")\n",
    "             .when((age_col >= 95), \"95+\")\n",
    "             .otherwise(\"invalid age\").alias(\"age\"))\n",
    "\n",
    "# Simple Merge function\n",
    "def user_bins_merge_data(df, batch_id):\n",
    "    (df.drop(\"updated\")\n",
    "     .createOrReplaceTempView(\"stream_updates\"))\n",
    "\n",
    "    df.sparkSession.sql(\"\"\"\n",
    "          MERGE INTO silver.user_bins a\n",
    "          USING stream_updates b\n",
    "          ON a.alt_id=b.alt_id\n",
    "          WHEN MATCHED THEN\n",
    "            UPDATE SET *\n",
    "          WHEN NOT MATCHED THEN\n",
    "            INSERT * \n",
    "          \"\"\")\n",
    "\n",
    "\n",
    "def process_user_bins():\n",
    "\n",
    "    (spark.sql(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS\n",
    "      silver.user_bins\n",
    "       (alt_id   STRING,\n",
    "        user_id  INTEGER,\n",
    "        age      STRING,\n",
    "        sex      STRING,\n",
    "        gender   STRING,\n",
    "        city     STRING,\n",
    "        state    STRING,\n",
    "        zip      INTEGER) \n",
    "      USING DELTA\n",
    "    \"\"\"))\n",
    "\n",
    "    (spark.readStream\n",
    "          .option(\"skipChangeCommits\", \"true\")\n",
    "          .table(\"silver.users\")\n",
    "          .select(\"alt_id\",\n",
    "                  \"user_id\", \n",
    "                  age_bins(f.col(\"dob\")),\n",
    "                  \"sex\",\n",
    "                  \"gender\", \n",
    "                  \"city\", \n",
    "                  \"state\",\n",
    "                  \"zip\",\n",
    "                  \"updated\")\n",
    "          .withWatermark(\"updated\", \"30 seconds\")\n",
    "          .writeStream\n",
    "              .foreachBatch(user_bins_merge_data)\n",
    "              .option(\"checkpointLocation\", f\"{checkpoint_path}/user_bins\")\n",
    "              .outputMode(\"update\")\n",
    "              .trigger(processingTime='5 seconds')\n",
    "              .queryName(\"silver_user_bins\")\n",
    "              .start()\n",
    "              )\n",
    "\n",
    "def reprocess_user_bins():\n",
    "    dbutils.fs.rm(f\"{checkpoint_path}/user_bins\", True)\n",
    "\n",
    "    spark.sql(\"drop table if exists silver.user_bins\")\n",
    "\n",
    "    process_user_bins()\n",
    "\n",
    "time.sleep(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e7018f-bff8-4014-a2ce-4c4afea60326",
     "showTitle": true,
     "title": "Workout User Summary"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading Workout User Summary related functions...\")\n",
    "\n",
    "def process_workout_user_summary():\n",
    "\n",
    "  spark.sql(\"\"\"\n",
    "            DROP VIEW IF EXISTS gold.workout_user_summary;\n",
    "            \"\"\")\n",
    "\n",
    "  spark.sql(\"\"\"\n",
    "                CREATE VIEW gold.workout_user_summary AS (\n",
    "                SELECT\n",
    "                    wh.user_id,\n",
    "                    wh.min_heartrate,\n",
    "                    wh.avg_heartrate,\n",
    "                    wh.max_heartrate,\n",
    "                    wh.start_time,\n",
    "                    wh.end_time,\n",
    "                    ub.age,\n",
    "                    ub.sex,\n",
    "                    ub.gender,\n",
    "                    ub.city,\n",
    "                    ub.state,\n",
    "                    ub.zip\n",
    "                FROM (SELECT\n",
    "                          cw.user_id,\n",
    "                          ul.device_id,\n",
    "                          cw.session_id,\n",
    "                          MIN(hr.heartrate) AS min_heartrate,\n",
    "                          AVG(hr.heartrate) AS avg_heartrate,\n",
    "                          MAX(hr.heartrate) AS max_heartrate,\n",
    "                          MIN(cw.start_time) AS start_time,\n",
    "                          MAX(cw.end_time) AS end_time\n",
    "                      FROM silver.completed_workouts cw\n",
    "                      JOIN silver.user_lookup ul ON cw.user_id = ul.user_id\n",
    "                      JOIN silver.heart_rate hr ON ul.device_id = hr.device_id\n",
    "                      GROUP BY cw.user_id, ul.device_id, cw.session_id\n",
    "                ) wh\n",
    "                JOIN silver.user_bins ub ON wh.user_id = ub.user_id\n",
    "                );\n",
    "            \"\"\")\n",
    "\n",
    "time.sleep(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f07e79-1528-42ef-afaf-fe6187254c12",
     "showTitle": true,
     "title": "Deletes"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading Deletes related functions...\")\n",
    "\n",
    "def broadcast_deletes(microBatchDF, batchId):\n",
    "    (microBatchDF\n",
    "        .filter(\"_change_type = 'delete'\")\n",
    "        .createOrReplaceTempView(\"deletes\"))\n",
    "    \n",
    "    microBatchDF.sparkSession.sql(\"\"\"\n",
    "        MERGE INTO silver.workouts w\n",
    "        USING deletes d\n",
    "        ON w.user_id = d.user_id\n",
    "        WHEN MATCHED\n",
    "            THEN DELETE\n",
    "    \"\"\")\n",
    "    \n",
    "    microBatchDF.sparkSession.sql(\"\"\"\n",
    "        MERGE INTO silver.completed_workouts cw\n",
    "        USING deletes d\n",
    "        ON cw.user_id = d.user_id\n",
    "        WHEN MATCHED\n",
    "            THEN DELETE\n",
    "    \"\"\")\n",
    "    \n",
    "    microBatchDF.sparkSession.sql(\"\"\"\n",
    "        MERGE INTO silver.heart_rate hr\n",
    "        USING deletes d\n",
    "        ON hr.device_id = d.device_id\n",
    "        WHEN MATCHED\n",
    "            THEN DELETE\n",
    "    \"\"\")\n",
    "    \n",
    "    microBatchDF.sparkSession.sql(\"\"\"\n",
    "        MERGE INTO silver.gym_logs gl\n",
    "        USING deletes d\n",
    "        ON gl.mac = d.mac_address\n",
    "        WHEN MATCHED\n",
    "            THEN DELETE\n",
    "    \"\"\")\n",
    "    \n",
    "    microBatchDF.sparkSession.sql(\"\"\"\n",
    "        MERGE INTO silver.users u\n",
    "        USING deletes d\n",
    "        ON u.alt_id = d.alt_id\n",
    "        WHEN MATCHED\n",
    "            THEN DELETE\n",
    "    \"\"\")\n",
    "\n",
    "    microBatchDF.sparkSession.sql(\"\"\"\n",
    "        DELETE FROM silver.user_bins\n",
    "        WHERE user_id IN (SELECT user_id FROM deletes)\n",
    "    \"\"\")\n",
    "\n",
    "    microBatchDF.sparkSession.sql(\"\"\"\n",
    "        MERGE INTO silver.delete_requests dr\n",
    "        USING deletes d\n",
    "        ON d.alt_id = dr.alt_id\n",
    "        WHEN MATCHED\n",
    "          THEN UPDATE SET status = \"deleted\"\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def process_deletes():\n",
    "    #counting before purge\n",
    "    before_count_workouts = spark.table(\"silver.workouts\").count()\n",
    "    before_count_completed_workouts = spark.table(\"silver.completed_workouts\").count()\n",
    "    before_count_gym_logs = spark.table(\"silver.gym_logs\").count()\n",
    "    before_count_heart_rate = spark.table(\"silver.heart_rate\").count()\n",
    "    before_count_users = spark.table(\"silver.users\").count()\n",
    "    before_count_user_bins = spark.table(\"silver.user_bins\").count()\n",
    "\n",
    "    spark.sql(\"DELETE FROM silver.user_lookup WHERE alt_id IN (SELECT alt_id FROM silver.delete_requests WHERE status = 'requested')\")\n",
    "\n",
    "    (spark.readStream\n",
    "          .format(\"delta\")\n",
    "          .option(\"readChangeFeed\", \"true\")\n",
    "          .option(\"startingVersion\", start_version)\n",
    "          .table(\"silver.user_lookup\")\n",
    "          .writeStream\n",
    "          .foreachBatch(broadcast_deletes)\n",
    "          .outputMode(\"update\")\n",
    "          .option(\"checkpointLocation\", f\"{checkpoint_path}/deletes\")\n",
    "          .trigger(processingTime='5 seconds')\n",
    "          .start()\n",
    "          \n",
    "    )\n",
    "\n",
    "    #counting after purge\n",
    "    uptodate_count_workouts = spark.table(\"silver.workouts\").count()\n",
    "    uptodate_count_completed_workouts = spark.table(\"silver.completed_workouts\").count()\n",
    "    uptodate_count_gym_logs = spark.table(\"silver.gym_logs\").count()\n",
    "    uptodate_count_heart_rate = spark.table(\"silver.heart_rate\").count()\n",
    "    uptodate_count_users = spark.table(\"silver.users\").count()\n",
    "    uptodate_count_user_bins = spark.table(\"silver.user_bins\").count()\n",
    "\n",
    "    #printing values obtained before and how many were purged in each table\n",
    "    print(f\"Before broadcasting the deletes, the table Workouts had {before_count_workouts} rows. {before_count_workouts - uptodate_count_workouts} rows were deleted, remaining now {uptodate_count_workouts}\")\n",
    "    print(\" \")\n",
    "    print(f\"Before broadcasting the deletes, the table Completed_workouts had {before_count_completed_workouts} rows. {before_count_completed_workouts - before_count_completed_workouts} rows were deleted, remaining now {uptodate_count_completed_workouts}\")\n",
    "    print(\" \")\n",
    "    print(f\"Before broadcasting the deletes, the table Gym_logs had {before_count_gym_logs} rows. {before_count_gym_logs - uptodate_count_gym_logs} rows were deleted, remaining now {uptodate_count_gym_logs}\")\n",
    "    print(\" \")\n",
    "    print(f\"Before broadcasting the deletes, the table Heart_rate had {before_count_heart_rate} rows. {before_count_heart_rate - uptodate_count_heart_rate} rows were deleted, remaining now {uptodate_count_heart_rate}\")\n",
    "    print(\" \")\n",
    "    print(f\"Before broadcasting the deletes, the table Users had {before_count_users} rows. {before_count_users - uptodate_count_users} rows were deleted, remaining now {uptodate_count_users}\")\n",
    "    print(\" \")\n",
    "    print(f\"Before broadcasting the deletes, the table User_bins had {before_count_user_bins} rows. {before_count_user_bins - uptodate_count_user_bins} rows were deleted, remaining now {uptodate_count_user_bins}\")\n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "time.sleep(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "307ffcec-4368-443b-b5c2-cae097a7634b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def initialize_pipeline():\n",
    "  print(\"Initializing pipeline...\")\n",
    "  print(\"\\n\")\n",
    "  process_bronze()\n",
    "  print(\"Broze layer stream started\")\n",
    "  print(\"\\n\")\n",
    "  process_heart_rate()\n",
    "  process_workouts()\n",
    "  process_users()\n",
    "  process_completed_workouts()\n",
    "  process_user_bins()\n",
    "  print(\"Silver layer streams started\")\n",
    "  print(\"\\n\")\n",
    "  process_gym_reports()\n",
    "  process_workout_user_summary()\n",
    "  print(\"Gold layer Views created\")\n",
    "  print(\"\\n\")\n",
    "  print(\"You can start the files ingestion!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de92c20d-de96-4257-a230-0b89c79bb105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of all the files that will simulate a streaming pipeline\n",
    "stream_files = {\n",
    "1:[\n",
    "\"part-00000-tid-532491258944704650-a78b6d69-f3e1-42eb-aaae-c3f48e9d75c2-1663-1-c000.json\",\n",
    "\"part-00003-tid-532491258944704650-a78b6d69-f3e1-42eb-aaae-c3f48e9d75c2-1666-1-c000.json\",\n",
    "\"part-00004-tid-532491258944704650-a78b6d69-f3e1-42eb-aaae-c3f48e9d75c2-1667-1-c000.json\",\n",
    "\"part-00005-tid-532491258944704650-a78b6d69-f3e1-42eb-aaae-c3f48e9d75c2-1668-1-c000.json\"\n",
    "],2:[\n",
    "\"part-00004-tid-4809231750595173478-9733187f-843a-46c7-9f52-eab674ba7338-1697-1-c000.json\",\n",
    "\"part-00003-tid-4809231750595173478-9733187f-843a-46c7-9f52-eab674ba7338-1696-1-c000.json\",\n",
    "\"part-00000-tid-4809231750595173478-9733187f-843a-46c7-9f52-eab674ba7338-1693-1-c000.json\"\n",
    "],3:[\n",
    "\"part-00000-tid-111368985018146951-3a640041-7f7f-44fd-9286-cb2f49cefac7-1714-1-c000.json\",\n",
    "\"part-00003-tid-111368985018146951-3a640041-7f7f-44fd-9286-cb2f49cefac7-1717-1-c000.json\",\n",
    "\"part-00004-tid-111368985018146951-3a640041-7f7f-44fd-9286-cb2f49cefac7-1718-1-c000.json\"\n",
    "],4:[\n",
    "\"part-00004-tid-8664805335701107839-047009b2-38c4-4637-b0da-a59ea7ab4725-1739-1-c000.json\",\n",
    "\"part-00003-tid-8664805335701107839-047009b2-38c4-4637-b0da-a59ea7ab4725-1738-1-c000.json\",\n",
    "\"part-00000-tid-8664805335701107839-047009b2-38c4-4637-b0da-a59ea7ab4725-1735-1-c000.json\"\n",
    "],5:[\n",
    "\"part-00004-tid-3723893643168635313-3ff2fc91-919e-4b54-959d-2fd836e454c9-1760-1-c000.json\",\n",
    "\"part-00003-tid-3723893643168635313-3ff2fc91-919e-4b54-959d-2fd836e454c9-1759-1-c000.json\",\n",
    "\"part-00000-tid-3723893643168635313-3ff2fc91-919e-4b54-959d-2fd836e454c9-1756-1-c000.json\"\n",
    "],6:[\n",
    "\"part-00004-tid-7816484799722101425-436eec6b-6daf-4672-9848-df532124b3b0-1781-1-c000.json\",\n",
    "\"part-00003-tid-7816484799722101425-436eec6b-6daf-4672-9848-df532124b3b0-1780-1-c000.json\",\n",
    "\"part-00000-tid-7816484799722101425-436eec6b-6daf-4672-9848-df532124b3b0-1777-1-c000.json\"\n",
    "],7:[\n",
    "\"part-00004-tid-3715053498828480727-07fd4187-5f08-408f-a02d-8664bb11ef18-1802-1-c000.json\",\n",
    "\"part-00003-tid-3715053498828480727-07fd4187-5f08-408f-a02d-8664bb11ef18-1801-1-c000.json\",\n",
    "\"part-00000-tid-3715053498828480727-07fd4187-5f08-408f-a02d-8664bb11ef18-1798-1-c000.json\"\n",
    "],8:[\n",
    "\"part-00004-tid-4510170731639983217-d3c96e48-d62c-409d-8b67-05e3743d8036-1823-1-c000.json\",\n",
    "\"part-00003-tid-4510170731639983217-d3c96e48-d62c-409d-8b67-05e3743d8036-1822-1-c000.json\",\n",
    "\"part-00000-tid-4510170731639983217-d3c96e48-d62c-409d-8b67-05e3743d8036-1819-1-c000.json\"\n",
    "],9:[\n",
    "\"part-00004-tid-3313816597353595179-9e5499ff-e6a8-477c-9662-8cab09d9db89-1844-1-c000.json\",\n",
    "\"part-00003-tid-3313816597353595179-9e5499ff-e6a8-477c-9662-8cab09d9db89-1843-1-c000.json\",\n",
    "\"part-00002-tid-3313816597353595179-9e5499ff-e6a8-477c-9662-8cab09d9db89-1842-1-c000.json\",\n",
    "\"part-00001-tid-3313816597353595179-9e5499ff-e6a8-477c-9662-8cab09d9db89-1841-1-c000.json\",\n",
    "\"part-00000-tid-3313816597353595179-9e5499ff-e6a8-477c-9662-8cab09d9db89-1840-1-c000.json\"\n",
    "],10:[\n",
    "\"part-00004-tid-4501507592585151880-6844bec2-7404-4906-b353-a3d81d2e649e-1865-1-c000.json\",\n",
    "\"part-00003-tid-4501507592585151880-6844bec2-7404-4906-b353-a3d81d2e649e-1864-1-c000.json\",\n",
    "\"part-00002-tid-4501507592585151880-6844bec2-7404-4906-b353-a3d81d2e649e-1863-1-c000.json\",\n",
    "\"part-00001-tid-4501507592585151880-6844bec2-7404-4906-b353-a3d81d2e649e-1862-1-c000.json\",\n",
    "\"part-00000-tid-4501507592585151880-6844bec2-7404-4906-b353-a3d81d2e649e-1861-1-c000.json\"\n",
    "],11:[\n",
    "\"part-00004-tid-8408848988009657416-45d2e867-810d-4f61-8f47-2884a06d772b-1886-1-c000.json\",\n",
    "\"part-00003-tid-8408848988009657416-45d2e867-810d-4f61-8f47-2884a06d772b-1885-1-c000.json\",\n",
    "\"part-00002-tid-8408848988009657416-45d2e867-810d-4f61-8f47-2884a06d772b-1884-1-c000.json\",\n",
    "\"part-00001-tid-8408848988009657416-45d2e867-810d-4f61-8f47-2884a06d772b-1883-1-c000.json\",\n",
    "\"part-00000-tid-8408848988009657416-45d2e867-810d-4f61-8f47-2884a06d772b-1882-1-c000.json\"\n",
    "],12:[\n",
    "\"part-00004-tid-8034711223202012424-14bb1983-06be-4427-bc93-8df65840763b-1907-1-c000.json\",\n",
    "\"part-00003-tid-8034711223202012424-14bb1983-06be-4427-bc93-8df65840763b-1906-1-c000.json\",\n",
    "\"part-00002-tid-8034711223202012424-14bb1983-06be-4427-bc93-8df65840763b-1905-1-c000.json\",\n",
    "\"part-00001-tid-8034711223202012424-14bb1983-06be-4427-bc93-8df65840763b-1904-1-c000.json\",\n",
    "\"part-00000-tid-8034711223202012424-14bb1983-06be-4427-bc93-8df65840763b-1903-1-c000.json\"\n",
    "],13:[\n",
    "\"part-00004-tid-5354968751320088002-9ee8ebbf-1623-44cb-aa20-e4ffd32909a4-1928-1-c000.json\",\n",
    "\"part-00003-tid-5354968751320088002-9ee8ebbf-1623-44cb-aa20-e4ffd32909a4-1927-1-c000.json\",\n",
    "\"part-00002-tid-5354968751320088002-9ee8ebbf-1623-44cb-aa20-e4ffd32909a4-1926-1-c000.json\",\n",
    "\"part-00001-tid-5354968751320088002-9ee8ebbf-1623-44cb-aa20-e4ffd32909a4-1925-1-c000.json\",\n",
    "\"part-00000-tid-5354968751320088002-9ee8ebbf-1623-44cb-aa20-e4ffd32909a4-1924-1-c000.json\"\n",
    "],14:[\n",
    "\"part-00004-tid-3713058930943657627-6ffcec81-9f8e-42b9-bfc8-e73734efd13b-1949-1-c000.json\",\n",
    "\"part-00003-tid-3713058930943657627-6ffcec81-9f8e-42b9-bfc8-e73734efd13b-1948-1-c000.json\",\n",
    "\"part-00002-tid-3713058930943657627-6ffcec81-9f8e-42b9-bfc8-e73734efd13b-1947-1-c000.json\",\n",
    "\"part-00001-tid-3713058930943657627-6ffcec81-9f8e-42b9-bfc8-e73734efd13b-1946-1-c000.json\",\n",
    "\"part-00000-tid-3713058930943657627-6ffcec81-9f8e-42b9-bfc8-e73734efd13b-1945-1-c000.json\"\n",
    "],15:[\n",
    "\"part-00004-tid-7685515290006980378-aedd825a-885d-4d2e-9a77-4adc6fa048e1-1970-1-c000.json\",\n",
    "\"part-00003-tid-7685515290006980378-aedd825a-885d-4d2e-9a77-4adc6fa048e1-1969-1-c000.json\",\n",
    "\"part-00002-tid-7685515290006980378-aedd825a-885d-4d2e-9a77-4adc6fa048e1-1968-1-c000.json\",\n",
    "\"part-00001-tid-7685515290006980378-aedd825a-885d-4d2e-9a77-4adc6fa048e1-1967-1-c000.json\",\n",
    "\"part-00000-tid-7685515290006980378-aedd825a-885d-4d2e-9a77-4adc6fa048e1-1966-1-c000.json\"\n",
    "],16:[\n",
    "\"part-00000-tid-1258914159397947207-0c400917-b30c-4bcb-825f-bef3aa9893be-1987-1-c000.json\",\n",
    "\"part-00003-tid-1258914159397947207-0c400917-b30c-4bcb-825f-bef3aa9893be-1990-1-c000.json\",\n",
    "\"part-00004-tid-1258914159397947207-0c400917-b30c-4bcb-825f-bef3aa9893be-1991-1-c000.json\",\n",
    "\"part-00005-tid-1258914159397947207-0c400917-b30c-4bcb-825f-bef3aa9893be-1992-1-c000.json\"\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d42c3bc-074f-4681-87ca-cc6247c3739f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class streaming_factory:\n",
    "    def __init__(self, starting_batch=1, max_batch=16):\n",
    "        self.max_batch = max_batch\n",
    "        self.batch = starting_batch\n",
    "    \n",
    "    def start(self):\n",
    "        print(\"Starting the stream\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        time.sleep(0.7)\n",
    "\n",
    "        print(\"Stream is ready!\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(f\"#{self.batch} batch of files arriving\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for i in stream_files[self.batch]:\n",
    "          dbutils.fs.cp(f\"{source}/{i}\", f\"{files_path}\")\n",
    "        self.batch += 1\n",
    "        \n",
    "    \n",
    "    def load(self, continuous=False):\n",
    "                \n",
    "        if self.batch > self.max_batch:\n",
    "            print(\"Data source exhausted\", end=\"...\")\n",
    "            return\n",
    "        \n",
    "        if continuous:\n",
    "            print(\"Loading all files\", end=\"...\")\n",
    "            \n",
    "            for key in stream_files:\n",
    "                for i in stream_files[key]:\n",
    "                    dbutils.fs.cp(f\"{source}/{i}\", f\"{files_path}\")\n",
    "            \n",
    "            self.batch = self.max_batch+1\n",
    "            \n",
    "        else:\n",
    "            print(f\"Loading batch #{self.batch} to the daily stream\", end=\"...\")\n",
    "            key = self.batch\n",
    "\n",
    "            for i in stream_files[key]:\n",
    "              dbutils.fs.cp(f\"{source}/{i}\", f\"{files_path}\")\n",
    "          \n",
    "            time.sleep(90)\n",
    "            self.batch += 1\n",
    "\n",
    "None # Suppressing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e71342a-664a-4823-9f0d-bfd2f7b61e1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream=streaming_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549fdd5f-ef4b-43d4-a947-8ef106d8ee77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class StreamManager:\n",
    "    def __init__(self, files_path):\n",
    "        self.files_path = files_path\n",
    "\n",
    "    def stop_stream(self, stream_name):\n",
    "        \"\"\"Stop a single stream by name.\"\"\"\n",
    "        stopped = False\n",
    "        for stream in spark.streams.active:\n",
    "            if stream.name == stream_name:\n",
    "                stream.stop()\n",
    "                stream.awaitTermination()\n",
    "                print(f\"Stream {stream_name} stopped successfully.\")\n",
    "                stopped = True\n",
    "                break\n",
    "        if not stopped:\n",
    "            print(f\"No active stream found with the name: {stream_name}\")\n",
    "\n",
    "    def stop_all_streams(self):\n",
    "        \"\"\"Stop all active streams.\"\"\"\n",
    "        for stream in spark.streams.active:\n",
    "            stream.stop()\n",
    "            stream.awaitTermination()\n",
    "        print(\"All active streams stopped.\")\n",
    "\n",
    "    def remove_folder(self):\n",
    "        dbutils.fs.rm(self.files_path, True)\n",
    "        print(f\"Folder {self.files_path} removed.\")\n",
    "\n",
    "    def restart_environment(self):\n",
    "        \"\"\"Stop all active streams and remove the source folder.\"\"\"\n",
    "        self.stop_all_streams()\n",
    "        self.remove_folder()\n",
    "        print(\"Environment restarted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd8980e-1945-443b-a10d-a721739da4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "maintenance = StreamManager(files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19578d83-6c3a-4051-9381-ff96082ccb15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"All functions are loaded!\")\n",
    "time.sleep(0.3)\n",
    "print(\"\\n\")\n",
    "print(\"Ready to start the pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b06370e-9f26-4e01-931a-2a0a47fcca66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
